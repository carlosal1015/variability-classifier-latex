% Improvement: Use Bayesian formulation of GLS for period finding in order to get a complete posterior probability function for the period
% Computational considerations
% Period finding is hard
% Future work
% Comparison with results vom Kim et. al and Dubath et. al

In this work, we have seen the application of three common supervised Machine Learning algorithms --- SVMs, Random Forest and Gradient Boosted Trees --- to the classification of periodic, semi--periodic and aperiodic variables in the LMC based on their photometric variability in three different bands. The training set contained 32683 sources, thereunder 15995 long--periodic variables, 2402 Cepheids, 511 $\delta$-Scuti variables, 4470 RR--Lyrae, 3538 eclipsing binaries, 796 blue variables, 180 quasars and 4791 non--periodic variables. This undertaking involved the extraction of $192$ real--valued ad--hoc features which characterize the periodicity, variability amplitude or shape and general appearance of the (phase--folded) light curve, followed by the training, hyperparameter optimization and model evaluation for all three classifiers. Our results in sections \ref{sec:performance-svm}, \ref{sec:performance-rf} and \ref{sec:performance-gb} show that automated Machine Learning classification in general provides promising results in terms of precision and recall for the majority of both superclasses and subclasses in the data set. They are a suitable way to deal with the ever growing amount of data from modern astronomy surveys because they are both efficient and provide accurate results. Nevertheless, there are differences between the examined approaches when it comes to overall accuracy, training and optimization effort or prediction run--time.\\

In section \ref{sec:performance-svm} we have trained a Support Vector Machine (SVM) with the RBF kernel $\kernel_\text{RBF}$ and optimized the hyperparameters $C$ and $\gamma$ by doing a $5$-fold cross--validation on a grid, searching for the highest average, weighted $F_1$-score, where the weights are chosen such that classes with more observations are more important. The best score for superclass classification is $(97.25 \, \pm \, 0.25) \, \%$ with $C = 4.67 \cdot 10^2$ and $\gamma = 1.69 \cdot 10^{-3}$, while the best score for the subclasses is $(82.75 \, \pm \, 0.47) \%$ with $C = 9.83 \cdot 10^2$ and $\gamma = 9.22 \cdot 10^{-4}$. The error estimate is the standard deviation of the cross--validation folds and can be interpreted as an estimate for the generalization error, which is the deviations we would expect when confronting our classifier with unseen data. In the confusion matrix on page \pageref{tab:svm-confusion-matrix-superclasses} we can see that the long--periodic variables have the highest scores while quasars and Type II Cepheids have much lower scores. It is important to realizes that this could merely be a result of the choice of scoring metrics. Note that maximizing the average, weighted $F_1$-score is somewhat arbitrary in the sense that it would need to be adapted to the specific problem we are trying to solve. Say we were interested in identifying quasars in a big data set where quasars are outnumbered by other sources, then we would probably optimize for quasar precision since we don't really care about misclassification between the other sources, and \emph{might} get much better scores \emph{with respect to that specific metric} than in table \ref{tab:svm-confusion-matrix-superclasses}. After all, for every quasar we have 89 long--periodic variables in the data set. This simply means that looking at the confusion matrix does not yet justify the conclusion that there is no discriminative information in the features to distinguish the small--observation classes from the other classes. However, since we are trying to assess the overall classification performance for individual classifiers in this work, optimizing for average, weighted $F_1$-score seems like a decent choice. In the subclass classification confusion matrix for SVM on page \pageref{tab:svm-confusion-matrix-subclasses} we can see that most of the confusion happens more or less within a square around the true positive on the diagonal, which simply means that most misclassification happens within a common superclass. While this is not surprising , it is still nice to see, since ultimately sources of a common superclass are expected to be similar, which leads to similar features and consequently does not allow for a precise separation. In fact, this is what we observe for all classifier's subclass confusion matrices in this work. The second classifier we trained was a Random Forest model. Once again, we tried to find an optimal combination of the parameters $t$ and $m$ using $5$-fold cross--validation on a grid. While grid search provided a reasonably clear maximum for the SVM, this was not the case for Random Forest. The best scores for superclass classification was $(98.14 \, \pm \, 0.07) \, \%$ with $t = 1800$ and $m = 80$, and $(85.39 \, \pm \, 0.46) \, \%$ with $t = 1000$ and $m = 100$ for subclass classification. This is in agreement with \citet{kim2014}. Furthermore, we have also trained a model optimizing for quasar precision $\cdots$\todo{Add results for quasar selection}. Random forest provides an intrinsic feature importance ranking which is visualized in figure ...\todo{Add reference to feature importance ranking}. Gradient Boosted Trees was the last model we have trained. It's best performance for the superclasses was $(98.43 \, \pm \, 0.07) \, \%$ and $(86.30 \, \pm \, 0.37) \, \%$ for subclass classification.\\ 

When comparing the SVM results to the two other classifiers it's clear to see that Random Forest and Gradient Boosted Trees have higher scores than the Support Vector Machine, with differences of $\approx 1 \, \%$ \resp $3$-$4 \, \%$ for superclasses and subclasses. While SVM is deterministic in the sense that it yields the same results when training multiple times on the same training data and testing on the same testing data, this is not the case for Random Forest. That is to say that we would expect some deviations when running the Random Forest classifier multiple times, but still, the SVM scores is reasonably far away that it seems fair to say that the decision tree based models outperform the Support Vector Machine. This is not that easy to say when it comes to the performance comparison of Random Forest and Gradient Boost Trees. The Gradient Boosted Trees model has better scores by $0.3 \, \%$ (superclasses) and $0.9 \, \%$, but this could this could be due to statistical fluctuations. One way simple way to find out if the improvement is statistically significant would be to estimate the variation by running the training and testing process multiple times, say at least $N = 20$ times with the same training and testing sets. Needless to say, in any case our performance estimates are robust enough to conclude that Gradient Boosted Trees can at least compete with the Random Forest algorithm for this specific task.

% The training set is based on known OGLE variable stars. At some point OGLE misclassification starts to play a role...

\todo[inline]{Compare results for all classifiers}
\todo{Take a look at common misclassifications}
\todo[inline]{Advantages and disadvantages of SVM}
\todo[inline]{Advantages and disadvantages of RF}
\todo[inline]{Advantages and disadvantages of RF}

\todo[inline]{Comment on the features used}

Nevertheless, we have to keep the limitations in mind...\todo{Elaborate on sampling cadence and less data points for Gaia}

\todo[inline]{Future work}