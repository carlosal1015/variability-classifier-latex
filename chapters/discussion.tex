% Computational considerations
% Period finding is hard
% Future work
% Comparison with results vom Kim et. al and Dubath et. al

In this work, we have seen the application of three common supervised Machine Learning algorithms --- SVMs, Random Forest and Gradient Boosted Trees --- to the classification of periodic, semi--periodic and aperiodic variables in the LMC based on their photometric variability in three different bands. The training set contained 32683 sources, thereunder 15995 long--periodic variables, 2402 Cepheids, 511 $\delta$-Scuti variables, 4470 RR--Lyrae, 3538 eclipsing binaries, 796 blue variables, 180 quasars and 4791 non--periodic variables. This undertaking involved the extraction of $192$ real--valued ad--hoc features which characterize the periodicity, variability amplitude or shape and general appearance of the (phase--folded) light curve, followed by the training, hyperparameter optimization and model evaluation for all three classifiers. Our results in sections \ref{sec:performance-svm}, \ref{sec:performance-rf} and \ref{sec:performance-gb} show that automated Machine Learning classification in general provides promising results in terms of precision and recall for the majority of both superclasses and subclasses in the data set. They are a suitable way to deal with the ever growing amount of data from modern astronomy surveys because they are both efficient and provide accurate results. Nevertheless, there are differences between the examined approaches when it comes to overall accuracy, training and optimization effort or prediction run--time.\\

In section \ref{sec:performance-svm} we have trained a Support Vector Machine (SVM) with the RBF kernel $\kernel_\text{RBF}$ and optimized the hyperparameters $C$ and $\gamma$ by doing a $5$-fold cross--validation on a grid, searching for the highest average, weighted $F_1$-score, where the weights are chosen such that classes with more observations are more important. The best score for superclass classification was $(97.25 \, \pm \, 0.25) \, \%$ with $C = 4.67 \cdot 10^2$ and $\gamma = 1.69 \cdot 10^{-3}$, while the best score for the subclasses was $(82.75 \, \pm \, 0.47) \%$ with $C = 9.83 \cdot 10^2$ and $\gamma = 9.22 \cdot 10^{-4}$. The error estimate is the standard deviation of the cross--validation folds and can be interpreted as an estimate for the generalization error, which is the deviations we would expect when confronting our classifier with unseen data. In the confusion matrix on page \pageref{tab:svm-confusion-matrix-superclasses} we can see that the long--periodic variables have the highest scores while quasars and Type II Cepheids have much lower scores. It is important to realizes that this could merely be a result of the choice of scoring metrics. Note that maximizing the average, weighted $F_1$-score is somewhat arbitrary in the sense that it would need to be adapted to the specific problem we are trying to solve. Say we were interested in identifying quasars in a big data set where quasars are outnumbered by other sources, then we would probably optimize for quasar precision since we don't really care about misclassification between the other sources, and \emph{might} get much better scores \emph{with respect to that specific metric} than in table \ref{tab:svm-confusion-matrix-superclasses}. After all, for every quasar we have 89 long--periodic variables in the data set. This simply means that looking at the confusion matrix does not yet justify the conclusion that there is no discriminative information in the features to distinguish the small--observation classes from the other classes. However, since we are trying to assess the overall classification performance for individual classifiers in this work, optimizing for average, weighted $F_1$-score seems like a decent choice. In the subclass classification confusion matrix for SVM on page \pageref{tab:svm-confusion-matrix-subclasses} we can see that most of the confusion happens more or less within a square around the true positive on the diagonal, which simply means that most misclassification happens within a common superclass. While this is not surprising , it is still nice to see, since ultimately sources of a common superclass are expected to be similar, which leads to similar features and consequently does not allow for a precise separation in feature space. In fact, this is what we observe for all classifier's subclass confusion matrices in this work. The second classifier we trained was a Random Forest model. Once again, we tried to find an optimal combination of the parameters $t$ and $m$ using $5$-fold cross--validation on a grid. While grid search provided a reasonably clear maximum for the SVM, this was not the case for Random Forest. The best scores for superclass classification was $(98.14 \, \pm \, 0.07) \, \%$ with $t = 1800$ and $m = 80$, and $(85.39 \, \pm \, 0.46) \, \%$ with $t = 1000$ and $m = 100$ for subclass classification. This is in agreement with \citet{kim2014}. Furthermore, we have also trained a model optimizing for quasar precision $\cdots$\todo{Add results for quasar selection}. Random forest provides an intrinsic feature importance ranking which is visualized in figure ...\todo{Add reference to feature importance ranking}. The third model we have trained was the Gradient Boosted Trees classifier. It's best performance for the superclasses was $(98.43 \, \pm \, 0.07) \, \%$ and $(86.30 \, \pm \, 0.37) \, \%$ for subclass classification.\todo{Mention optimization process for GBT}\\

When comparing the SVM results to the two other classifiers it's clear to see that Random Forest and Gradient Boosted Trees have higher scores than the Support Vector Machine, with differences of about $1 \, \%$ \resp $3$-$4 \, \%$ for superclasses and subclasses. While SVM is deterministic in the sense that it yields the same results when training multiple times on the same training data and testing on the same testing data, this is not the case for Random Forest. That is to say that we would expect some deviations when running the Random Forest classifier multiple times, but still, the SVM scores are reasonably far away that it seems fair to say that the decision tree based models outperform the Support Vector Machine. This is not that easy to say when it comes to the performance comparison of Random Forest and Gradient Boost Trees. The Gradient Boosted Trees model has better scores by $0.3 \, \%$ (superclasses) and $0.9 \, \%$, but this could be due to statistical fluctuations. One simple way to find out if the improvement is statistically significant would be to estimate the variation by running the training and testing process multiple times, say at $N \ge 20$ times, with the same training and testing sets. Needless to say, in any case our performance estimates are robust enough to conclude that Gradient Boosted Trees can at least compete with the Random Forest algorithm for this specific task.\\

The most important feature we have extracted was $T_\text{LS}$, the Lomb--Scargle period. By visual inspection of a random subset of the phase--folded light curves, we find that the Lomb--Scargle algorithm provides accurate results for a good part of the classes, especially for Cepheids, RR Lyrae and eclipsing binaries with clear evidence for periodicity. However, for variables with low signal--to--noise ratio, short periods ($\delta$-Scuti), semi--periodic (long--periodic variables) or aperiodic (non--variables) behaviour, the extracted period is often spurious, \eg corresponding to multiples of the mean solar day $1.00274 \, \unit{d}$, or suspiciously close to the solar year $365.24 \, \unit{d}$ or multiples of it. For faint sources with small variability amplitudes, the high photometric uncertainty makes it very hard to detect statistically significant evidence for periodicity at all. This is to some extent mitigated by the fact that these sources will have a high false--alarm probability $\text{FAP}_\text{LS}$. When inspecting the phase--folds for eclipsing binaries we find that there are some problems with period \emph{aliases}, where in most cases half the true period seems to be extracted. Looking at the periods extracted by the CE algorithm, we find that...\\

% Aliases

% The training set is based on known OGLE variable stars. At some point OGLE misclassification starts to play a role...

\todo[inline]{Compare results for all classifiers}
\todo{Take a look at common misclassifications}
\todo[inline]{Advantages and disadvantages of SVM}
\todo[inline]{Advantages and disadvantages of RF}
\todo[inline]{Advantages and disadvantages of RF}

\todo[inline]{Comment on the features used}

% For the features: In the end domain knowledge is really important. Could use more feature engineering...
% The structure function provides decent separation for quasars on the $A$-$\gamma$-plane, as seen in \ref{...}. The most contamination comes from RR Lyrae...

% Performance considerations

Still, we have to keep the limitations in mind. The EROS project was a microlensing survey with relatively well--sampled light curves. The number of data points per light curve are somewhere between 200 and 300. However, this can not be achieved in surveys likes Gaia due to different goals in mind. When the number of data points drops, the overall performance will also decrease in general, since we have less evidence available pointing towards one or the other class.\todo{Elaborate on sampling cadence and less data points for Gaia}\\

% Furthermore, as Gaia will include observations of much fainter stars than previous surveys, 

\todo[inline]{Future work}
In future work...
\begin{itemize}
 
% http://www.jmlr.org/papers/volume3/guyon03a/guyon03a.pdf
\item In this work we have extracted 64 features within $B_R, B_B$ and $B_{R-B}$. Some of these 192 features are highly correlated --- or even identical in some cases --- and could be safely removed from the extraction process to speed up feature extraction reduce training time without affecting the overall classification scores. While this may sound straightforward, it s harder...\todo{Continue}. There are a variety of methods to do this. The simplest method would be to use the ...\todo{Continue + cite features selection paper.}. Another viable way to deal with this would be to keep all 192 features and extract $k$ dominant components\footnote{Dominant components with respect to the variance in feature space. $k$ would be another hyperparameter for the model.} using principal component analysis (PCA) to project the features into a lower--dimensional subspace using an orthogonal transformation prior to training (and testing). This would not decrease the feature extraction time, but it would certainly cut training time.
 
\item Bayesian Multiband Generalized Lomb--Scargle\improvement{Use Bayesian formulation of GLS for period finding in order to get a complete posterior probability function for the period}
% Machine Learning approaches for period--finding

\item One immediate application of this work's findings would be to run the classifiers on survey--specific data, \eg on Gaia data, and see how this affects the overall classification performance. This would involve the downsampling of the EROS light curves to the respective sampling cadence and a well--grounded adjustment of the photometric uncertainties.

\item We would be interested in an exhaustive, systematic comparison of different period--finding algorithms based on a training set with \emph{a priori} knowledge about the underlying periodicity, similar to \citet{graham2013b}. This training set could both contain simulated light curves, as well as real--world data with periods provided by domain--experts after the visual inspection of the light curves.

\item There are a variety of promising Machine Learning methods availabe for period--finding, such as...

\end{itemize}