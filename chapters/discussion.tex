In this work, we have seen the application of three common supervised Machine Learning algorithms --- SVMs, Random Forests and Gradient Boosted Trees --- to the classification of periodic, semi--periodic and aperiodic variables in the LMC based on their photometric variability in three different bands. The training set contained 32683 sources, thereunder 15995 long--periodic variables, 2402 Cepheids, 511 $\delta$-Scuti variables, 4470 RR--Lyrae, 3538 eclipsing binaries, 796 blue variables, 180 quasars and 4791 non--periodic variables. This undertaking involved the extraction of $192$ real--valued ad--hoc features which characterize the periodicity, variability amplitude or shape and general appearance of the (phase--folded) light curve, followed by the training, hyperparameter optimization and model evaluation for all three classifiers. Our results in sections \ref{sec:performance-svm}, \ref{sec:performance-rf} and \ref{sec:performance-gb} show that automated Machine Learning classification in general provides promising results in terms of precision and recall for the majority of both superclasses and subclasses in the data set. They are a suitable way to deal with the ever growing amount of data from modern astronomy surveys. Nevertheless, there are differences between the examined approaches when it comes to overall accuracy, training and optimization effort or prediction run--time.\\

\subsubsection*{Performance of the individual classifiers}

In section \ref{sec:performance-svm} we have trained a Support Vector Machine (SVM) with the RBF kernel $\kernel_\text{RBF}$ and optimized the hyperparameters $C$ and $\gamma$ by doing a $5$-fold cross--validation on a grid, searching for the highest average, weighted $F_1$-score, where the weights are chosen such that classes with more observations are more important. The best score for superclass classification was $(97.25 \, \pm \, 0.25) \, \%$ with $C = 4.67 \cdot 10^2$ and $\gamma = 1.69 \cdot 10^{-3}$, while the best score for the subclasses was $(82.75 \, \pm \, 0.47) \%$ with $C = 9.83 \cdot 10^2$ and $\gamma = 9.22 \cdot 10^{-4}$. The error estimate is the standard deviation of the cross--validation folds and can be interpreted as an estimate for the generalization error, which is the deviation we would expect when confronting our classifier with unseen data. In the confusion matrix on page \pageref{tab:svm-confusion-matrix-superclasses} we can see that the long--periodic variables have the highest scores while quasars and Type II Cepheids have much lower scores. It is important to realizes that this could merely be a result of the choice of scoring metrics. Note that maximizing the average, weighted $F_1$-score is somewhat arbitrary in the sense that it would need to be adapted to the specific problem we are trying to solve. Say we were interested in identifying quasars in a big data set where quasars are outnumbered by other sources, then we would probably optimize for quasar precision since we don't really care about misclassification between the other sources, and \emph{could} get much better scores \emph{with respect to that specific metric}. After all, for every quasar we have 89 long--periodic variables in the data set. In fact, this is what we did (see below). This simply means that looking at the confusion matrix does not yet justify the conclusion that there is no discriminative information in the features to distinguish the small--observation classes from the other classes. However, since we are trying to assess the overall classification performance for individual classifiers in this work, optimizing for average, weighted $F_1$-score seems like a decent choice. In the subclass classification confusion matrix for SVM on page \pageref{tab:svm-confusion-matrix-subclasses} we can see that most of the confusion happens more or less within a square around the true positive on the diagonal, which simply means that most misclassification happens within a common superclass. While this is not surprising, it is still nice to see, since ultimately sources of a common superclass are expected to be similar, which leads to similar features and consequently does not allow for a precise separation in feature space. In fact, this is what we observe for all classifier's subclass confusion matrices in this work. The second classifier we trained was a Random Forest model. Once again, we tried to find an optimal combination of the hyperparameters $t$ and $m$ using $5$-fold cross--validation on a grid. While grid search provided a reasonably clear maximum for the SVM, this was not the case for Random Forest. The best scores for superclass classification were $(98.14 \, \pm \, 0.07) \, \%$ with $t = 1800$ and $m = 80$, and $(85.39 \, \pm \, 0.46) \, \%$ with $t = 1000$ and $m = 100$ for subclass classification. This is in agreement with \citet{kim2014}, although they got similar scores using fewer features. Furthermore, we have also trained a model optimizing for quasar precision and achieve $97.65 \, \%$ precision and $46.11 \, \%$ recall (see table \ref{tab:rf-confusion-matrix-qso}). Random forest provides an intrinsic feature importance ranking which is visualized in figure \ref{fig:rf-feature-importance}. The third model we have trained was the Gradient Boosted Trees classifier. Its best performance for the superclasses was $(98.43 \, \pm \, 0.07) \, \%$ ($t = 1000, m = 80$, learning rate 0.02) and $(86.30 \, \pm \, 0.37) \, \%$ for subclass classification $t = 6000, m = 75$ and a learning rate of 0.01.\\

\subsubsection*{Comparison of the classifiers}

When comparing the SVM results to the two other classifiers it is clear to see that Random Forest and Gradient Boosted Trees have higher scores than the Support Vector Machine, with differences of about $1 \, \%$ \resp $3$-$4 \, \%$ for superclasses and subclasses. While SVM is deterministic in the sense that it yields the same results when training multiple times on the same training data and testing on the same testing data, this is not the case for Random Forest. That is to say that we would expect some deviations when running the Random Forest classifier multiple times, but still, the SVM scores are reasonably far away that it seems fair to say that the decision tree based models outperform the Support Vector Machine. This is not that easy to say when it comes to the performance comparison of Random Forest and Gradient Boost Trees. The Gradient Boosted Trees model has better scores by $0.3 \, \%$ (superclasses) and $0.9 \, \%$, but this could be due to statistical fluctuations. One simple way to find out if the improvement is statistically significant would be to estimate the variation by running the training and testing process multiple times, say $N \ge 20$ times, with the same training and testing sets. Needless to say, in any case our performance estimates are robust enough to conclude that Gradient Boosted Trees can compete with the Random Forest algorithm for this specific task.\\

Apart from the classification performance, the models we have tested differ with respect to computational complexity and scalability, \ie the ability to deal with large data sets, as well as the preprocessing efforts required to get decent results. In some cases, people might also be interested in a model that is easy to interpret and provides some insight into the decision--making process, while others prefer to use models which are flexible enough to be able to incorporate further domain--knowledge. This shows that the ultimate choice will very often depend on multiple considerations, tailored to the \emph{specific} problem to be solved.\\

The Support Vector Machine is a solid classification algorithm with a straightforward hyperparameter optimization of $C$ and $\gamma$ due to the convex nature of the underlying optimization problem during training. It requires feature scaling as a preprocessing step, but this is not overly costly. With a proper choice of $\kernel$, it allows for non--linear classification, and at the same time can be used to incorporate domain--knowledge. Inherently, SVMs only work for binary classification problems, but this can be overcome by training multiple classifiers. Unfortunately, the obtained model is not easy to interpret or visualize, at least for higher--dimensional problems. This is in contrast to the decision tree based models, which are easier to interpret, \eg by inspecting the actual split criteria in the tree. Since the algorithm is \emph{embarrassingly parallel}\footnote{This means that it is easy to parallelize, since all trees of the forest are independent, and can be grown on dedicated cores.}, the Random Forest model is fast to train and is able to handle large data sets or number of trees. It provides a feature importance ranking which is convenient and serves as a first estimate for the relevance of the extracted features, although its results should not be overestimated, \eg because the scores are correlated. Finding the optimal number of trees $t$ and the optimal number of features $m$ to consider can be challenging, which is a drawback compared to SVM. The Gradient Boosted Trees classifier yielded similar scores than Random Forest, but the training process was \emph{a lot} slower because it can not be easily parallelized, since each classifier is an expert on the misclassification of its predecessor and consequently has to wait for it to finish. Furthermore, it is considerably harder to optimize the model's hyperparameter in a systematic way.

\subsubsection*{On the effectiveness of the features}

One of the most important features we have extracted was $T_\text{LS}$, the Lomb--Scargle period. By visual inspection of a random subset of the phase--folded light curves, we find that the Lomb--Scargle algorithm provides reasonable results for a good part of the classes, especially for Cepheids, RR Lyrae and eclipsing binaries with clear evidence for periodicity. However, for variables with low signal--to--noise ratio, short periods ($\delta$-Scuti), semi--periodic (long--periodic variables) or aperiodic (non--variables) behaviour, the extracted period is often spurious, \eg corresponding to multiples of the mean solar day $1.00274 \, \unit{d}$, or suspiciously close to the solar year $365.24 \, \unit{d}$ or multiples of it. For faint sources with small variability amplitudes, the high photometric uncertainty makes it very hard to detect statistically significant evidence for periodicity at all. Concerning classification, this is to some extent mitigated by the fact that these sources will have a high false--alarm probability $\text{FAP}_\text{LS}$. When inspecting the phase--folds for eclipsing binaries we find that there are some problems with period \emph{aliases}, where in most cases half the true period seems to be extracted because the two eclipses are superimposed. Looking at the periods extracted by the CE algorithm, we find that the overall performance is worse compared to Lomb--Scargle, except for strong signals from Cepheids, RR Lyrae and eclipsing binaries where it provides similar results. Especially long--periodic variables suffer from a generally lower conditional entropy in a broad band around (multiples of) $365.24 \, \unit{d}$. Furthermore, we find that there is a systematic trend in the periodograms towards lower conditional entropy for higher periods. This is in agreement with \citet{graham2013}, and can be corrected for \eg by using a median filter, although that would not improve the preceding. The original authors claim that the CE algorithm is less prone to aliasing issues. This is in contrast to our findings, being confronted with a lot multiples of $1.00274 \, \unit{d}$. In a second, careful read of \citet{graham2013}, we find that they refer to a variant of the algorithm where they extract $k$ candidate periods with the lowest conditional entropy, and then use \emph{analysis of variance} (AVO)\footnote{This is another period--finding algorithm based on phase--dispersion minimization.} \citep{schwarzenberg1999} to extract the best period according to AVO. We decided not to follow this approach, but instead mask multiples of the solar day in the periodogram. This can be problematic for universal period--finding, but might be sufficient in our case, since we still have Lomb--Scargle, and are not primarily interested period--finding. Period--finding is a \emph{nontrivial} problem, and there is always a trade--off between accuracy and computational effort.\\

The structure function provides a decent separation in $\gamma$, as seen in figure \ref{fig:features-structure-function}. Other important features are $\eta, \eta^\phi, \Sigma$ and $\Sigma^\phi$ as we can see from the scatterplots on page \pageref{fig:features-scatterplot}. For Random Forest, the feature importance ranking in figure \ref{fig:rf-feature-importance}, suggests that $Q_{25}$ ($B_E$), $Q_{25}^\phi$ ($B_E$) and $Q_{25}$ ($R_E - B_E$) are important variability features, as well.\\

Altogether, we have extracted 64 features within $R_E, B_E$ and $R_E - B_E$. Some of these 192 features are highly correlated --- or even identical in some cases --- and could be safely removed from the extraction process to speed up feature extraction reduce training time without affecting the overall classification scores. There are a variety of methods to do \emph{feature selection}, such as using information--theoretic ranking criteria or unsupervised Machine Learning methods \citep{guyon2003}. Another viable way to deal with this would be to keep all 192 features and extract $k$ dominant components\footnote{Dominant components with respect to the variance in feature space. $k$ would be another hyperparameter for the model.} using principal component analysis (PCA) to project the features into a lower--dimensional subspace using an orthogonal transformation prior to training (and testing). This would not decrease the feature extraction time, but it would certainly cut training time.\\

Ultimately, the performance of even the best Machine Learning algorithms will not be any better than guessing if the features do not contain meaningful information concerning class separation. This illustrates the importance of domain knowledge when devising features for classification.

%\subsubsection*{Limitations}

% http://www.jmlr.org/papers/volume3/guyon03a/guyon03a.pdf
%Still, we have to keep the limitations in mind, for example due to...\todo{Limitations of what?}. The EROS project was a microlensing survey with relatively well--sampled light curves. The number of data points per light curve are somewhere between 200 and 300. However, this can not be achieved in surveys likes Gaia due to different goals in mind. When the number of data points drops, the overall performance will also decrease in general, since we have less evidence available pointing towards one or the other class. Furthermore, further investigations would be necessary when trying to classify sources outside of the LMC, particularly regarding the relative frequencies of observations in the training set.\\

% Furthermore, as Gaia will include observations of much fainter stars than previous surveys,

\subsubsection*{Future work}

The results of this work could serve as a starting point for further investigations and improvements. A lot of the methods discussed, \eg period--finding, are applicable to a broad variety of problems in peripheral fields, not only limited to astronomy. In particular, it would be interesting to work on the following problems:
\begin{itemize}

\item One immediate application of this work's findings would be to run the same classifiers on survey--specific data, \eg Gaia data. We could make use of the EROS training set, simulating Gaia--like data by down--sampling the light curves to the respective sampling cadence and a well--grounded adjustment of the photometric uncertainties. Certainly, this can only be seen as an approximation, since the actual scanning--law is more complicated than this and we would expect differences in colours and magnitudes, too. We could then rerun the feature extraction on the generated light curves, and train and optimize the models to get an idea how the $\approx 70 \, \%$ drop in data points affects the overall classification performance.

\item We would be interested in an exhaustive, systematic comparison of different period--finding algorithms based on a training set with \emph{a priori} knowledge about the underlying periodicity, similar to \citet{graham2013b}. This training set could both contain simulated light curves, as well as real--world data with periods provided by domain--experts after the visual inspection of the light curves.

\item \citet{mortier2015} provide a Bayesian formulation of the Generalized Lomb--Scargle algorithm (BGLS) which yields a complete posterior probability. \citet{vanderplas2015} introduced a \emph{multiband} periodogram based on Lomb--Scargle. We would like to see a combination of both, \ie a Bayesian formulation of the Generalized Lomb--Scargle for multiband surveys.

\end{itemize}
